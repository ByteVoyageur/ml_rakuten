{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2802567-a300-4d5e-857e-9f80c68dd082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c6c4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>prdtypecode</th>\n",
       "      <th>designation_cleaned</th>\n",
       "      <th>description_cleaned</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>dup_count</th>\n",
       "      <th>is_duplicated_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>10</td>\n",
       "      <td>olivia: personalisiertes notizbuch 150 seiten ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>olivia: personalisiertes notizbuch 150 seiten ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>2280</td>\n",
       "      <td>journal arts (le) n° 133 28/09/2001 l'art marc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>journal arts (le) n° 133 28/09/2001 l'art marc...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>50</td>\n",
       "      <td>grand stylet ergonomique bleu gamepad nintendo...</td>\n",
       "      <td>pilot style touch pen marque speedlink stylet ...</td>\n",
       "      <td>grand stylet ergonomique bleu gamepad nintendo...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>1280</td>\n",
       "      <td>peluche donald europe disneyland 2000 (marionn...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>peluche donald europe disneyland 2000 (marionn...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>2705</td>\n",
       "      <td>guerre tuques</td>\n",
       "      <td>luc idées grandeur veut organiser jeu guerre b...</td>\n",
       "      <td>guerre tuques luc idées grandeur veut organise...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    productid     imageid  prdtypecode  \\\n",
       "0  3804725264  1263597046           10   \n",
       "1   436067568  1008141237         2280   \n",
       "2   201115110   938777978           50   \n",
       "3    50418756   457047496         1280   \n",
       "4   278535884  1077757786         2705   \n",
       "\n",
       "                                 designation_cleaned  \\\n",
       "0  olivia: personalisiertes notizbuch 150 seiten ...   \n",
       "1  journal arts (le) n° 133 28/09/2001 l'art marc...   \n",
       "2  grand stylet ergonomique bleu gamepad nintendo...   \n",
       "3  peluche donald europe disneyland 2000 (marionn...   \n",
       "4                                      guerre tuques   \n",
       "\n",
       "                                 description_cleaned  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  pilot style touch pen marque speedlink stylet ...   \n",
       "3                                                NaN   \n",
       "4  luc idées grandeur veut organiser jeu guerre b...   \n",
       "\n",
       "                                        text_cleaned  dup_count  \\\n",
       "0  olivia: personalisiertes notizbuch 150 seiten ...          1   \n",
       "1  journal arts (le) n° 133 28/09/2001 l'art marc...          1   \n",
       "2  grand stylet ergonomique bleu gamepad nintendo...          1   \n",
       "3  peluche donald europe disneyland 2000 (marionn...          1   \n",
       "4  guerre tuques luc idées grandeur veut organise...          1   \n",
       "\n",
       "   is_duplicated_group  \n",
       "0                False  \n",
       "1                False  \n",
       "2                False  \n",
       "3                False  \n",
       "4                False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"rakuten_text_train_v1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d2f704",
   "metadata": {},
   "source": [
    "# ============================================\n",
    "# 2. Fonctions utilitaires pour les features\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e3bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme du DataFrame : (84916, 8)\n",
      "                                 designation_cleaned  \\\n",
      "0  olivia: personalisiertes notizbuch 150 seiten ...   \n",
      "1  journal arts (le) n° 133 28/09/2001 l'art marc...   \n",
      "2  grand stylet ergonomique bleu gamepad nintendo...   \n",
      "3  peluche donald europe disneyland 2000 (marionn...   \n",
      "4                                      guerre tuques   \n",
      "\n",
      "                                 description_cleaned  \\\n",
      "0                                                      \n",
      "1                                                      \n",
      "2  pilot style touch pen marque speedlink stylet ...   \n",
      "3                                                      \n",
      "4  luc idées grandeur veut organiser jeu guerre b...   \n",
      "\n",
      "                                        text_cleaned  \n",
      "0  olivia: personalisiertes notizbuch 150 seiten ...  \n",
      "1  journal arts (le) n° 133 28/09/2001 l'art marc...  \n",
      "2  grand stylet ergonomique bleu gamepad nintendo...  \n",
      "3  peluche donald europe disneyland 2000 (marionn...  \n",
      "4  guerre tuques luc idées grandeur veut organise...  \n"
     ]
    }
   ],
   "source": [
    "# Toutes les colonnes texte utilisées par TF-IDF doivent être des chaînes non nulles\n",
    "text_cols_base = [\"designation_cleaned\", \"description_cleaned\", \"text_cleaned\"]\n",
    "for col in text_cols_base:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(\"\").astype(str)\n",
    "\n",
    "# S'assurer que les colonnes de duplication sont numériques\n",
    "for col in [\"dup_count\", \"is_duplicated_group\"]:\n",
    "    if col in df.columns:\n",
    "        # Conversion robuste en numérique, puis remplacement des NaN par 0\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "print(\"Forme du DataFrame :\", df.shape)\n",
    "print(df[text_cols_base].head())\n",
    "\n",
    "def safe_str(s):\n",
    "    \"\"\"Convertit une valeur en chaîne de caractères sûre (sans NaN).\"\"\"\n",
    "    if isinstance(s, str):\n",
    "        return s\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    return str(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4d0d5",
   "metadata": {},
   "source": [
    "# ---------- 2.1 Statistiques de base sur le texte ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ff4d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_text_stats(s: str) -> dict:\n",
    "    \"\"\"Calcule des statistiques simples sur une chaîne de caractères.\"\"\"\n",
    "    s = safe_str(s)\n",
    "    s_strip = s.strip()\n",
    "    \n",
    "    length_char = len(s_strip)\n",
    "    tokens = s_strip.split()\n",
    "    length_tokens = len(tokens)\n",
    "\n",
    "    num_digits = sum(ch.isdigit() for ch in s_strip)\n",
    "    num_upper = sum(ch.isupper() for ch in s_strip)\n",
    "    num_punct = sum(ch in \".,;:!?/\\\\-+()\" for ch in s_strip)\n",
    "\n",
    "    ratio_digits = num_digits / length_char if length_char > 0 else 0.0\n",
    "    ratio_upper = num_upper / length_char if length_char > 0 else 0.0\n",
    "\n",
    "    size_pattern = re.compile(r\"\\b\\d+\\s*(cm|mm|ml|l|kg|g)\\b\", flags=re.IGNORECASE)\n",
    "    num_size = len(size_pattern.findall(s_strip))\n",
    "\n",
    "    return {\n",
    "        \"len_char\": length_char,\n",
    "        \"len_tokens\": length_tokens,\n",
    "        \"ratio_digits\": ratio_digits,\n",
    "        \"ratio_upper\": ratio_upper,\n",
    "        \"num_punct\": num_punct,\n",
    "        \"num_size_patterns\": num_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bdae2d",
   "metadata": {},
   "source": [
    "# ---------- 2.2 Features \"pragmatiques\" sur le texte ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fcf833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLAMATION_RE = re.compile(r\"!\")\n",
    "QUESTION_RE = re.compile(r\"\\?\")\n",
    "PLUS_RE = re.compile(r\"\\+\")\n",
    "SLASH_RE = re.compile(r\"/\")\n",
    "\n",
    "# Petites listes de mots pour le matériau et la couleur\n",
    "MATERIAL_WORDS = {\"bois\", \"metal\", \"métal\", \"acier\", \"coton\", \"plastique\", \"cuir\"}\n",
    "COLOR_WORDS = {\"rouge\", \"bleu\", \"noir\", \"blanc\", \"rose\", \"gris\", \"vert\", \"jaune\"}\n",
    "\n",
    "def meta_features_from_text(s: str) -> dict:\n",
    "    \"\"\"Extrait des features \"pragmatiques\" d'un texte nettoyé.\"\"\"\n",
    "    s = safe_str(s)\n",
    "    s_lower = s.lower()\n",
    "    tokens = s_lower.split()\n",
    "    \n",
    "    return {\n",
    "        \"num_exclamation\": len(EXCLAMATION_RE.findall(s)),\n",
    "        \"num_question\": len(QUESTION_RE.findall(s)),\n",
    "        \"num_plus\": len(PLUS_RE.findall(s)),\n",
    "        \"num_slash\": len(SLASH_RE.findall(s)),\n",
    "        \"num_material_words\": sum(word in MATERIAL_WORDS for word in tokens),\n",
    "        \"num_color_words\": sum(word in COLOR_WORDS for word in tokens),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ef4a3",
   "metadata": {},
   "source": [
    "# ---------- 2.3 Features \"produit\" simples ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc4b38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_level_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Crée des features simples au niveau produit (description présente, longueur, etc.).\"\"\"\n",
    "    # Ici, on n'a plus la colonne 'description' brute, donc on\n",
    "    # utilise 'description_cleaned' comme approximation.\n",
    "    df[\"has_description\"] = df[\"description_cleaned\"].fillna(\"\").apply(\n",
    "        lambda s: int(len(safe_str(s).strip()) > 0)\n",
    "    )\n",
    "\n",
    "    df[\"desc_len_tokens_raw\"] = df[\"description_cleaned\"].fillna(\"\").apply(\n",
    "        lambda s: len(safe_str(s).split())\n",
    "    )\n",
    "\n",
    "    # Seuil arbitraire, à ajuster après EDA si besoin\n",
    "    df[\"desc_is_long\"] = (df[\"desc_len_tokens_raw\"] > 50).astype(int)\n",
    "\n",
    "    df[\"title_has_digit\"] = df[\"designation_cleaned\"].fillna(\"\").apply(\n",
    "        lambda s: int(any(ch.isdigit() for ch in safe_str(s)))\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ef6289",
   "metadata": {},
   "source": [
    "# ============================================\n",
    "# 3. Application des features de structure\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "583b87a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes après ajout des features :\n",
      "Index(['productid', 'imageid', 'prdtypecode', 'designation_cleaned',\n",
      "       'description_cleaned', 'text_cleaned', 'dup_count',\n",
      "       'is_duplicated_group', 'designation_cleaned_len_char',\n",
      "       'designation_cleaned_len_tokens', 'designation_cleaned_ratio_digits',\n",
      "       'designation_cleaned_ratio_upper', 'designation_cleaned_num_punct',\n",
      "       'designation_cleaned_num_size_patterns', 'description_cleaned_len_char',\n",
      "       'description_cleaned_len_tokens', 'description_cleaned_ratio_digits',\n",
      "       'description_cleaned_ratio_upper', 'description_cleaned_num_punct',\n",
      "       'description_cleaned_num_size_patterns', 'num_exclamation',\n",
      "       'num_question', 'num_plus', 'num_slash', 'num_material_words',\n",
      "       'num_color_words', 'has_description', 'desc_len_tokens_raw',\n",
      "       'desc_is_long', 'title_has_digit'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# --- 3.1 Statistiques de base sur titre et description ---\n",
    "for col in [\"designation_cleaned\", \"description_cleaned\"]:\n",
    "    stats_series = df[col].apply(basic_text_stats)\n",
    "    stats_df = pd.DataFrame(list(stats_series))\n",
    "    for stat_col in stats_df.columns:\n",
    "        new_col_name = f\"{col}_{stat_col}\"\n",
    "        df[new_col_name] = stats_df[stat_col]\n",
    "\n",
    "# --- 3.2 Features pragmatiques sur text_cleaned ---\n",
    "meta_series = df[\"text_cleaned\"].apply(meta_features_from_text)\n",
    "meta_df = pd.DataFrame(list(meta_series))\n",
    "for c in meta_df.columns:\n",
    "    df[c] = meta_df[c]\n",
    "\n",
    "# --- 3.3 Features produit ---\n",
    "df = product_level_features(df)\n",
    "\n",
    "print(\"Colonnes après ajout des features :\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4996483",
   "metadata": {},
   "source": [
    "# ============================================\n",
    "# 4. Préparation des listes de features\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfed8648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de colonnes numériques sélectionnées : 24\n",
      "Forme de X : (84916, 26)\n"
     ]
    }
   ],
   "source": [
    "# Colonnes textuelles pour TF-IDF (titre et description séparés)\n",
    "text_cols = [\"designation_cleaned\", \"description_cleaned\"]\n",
    "\n",
    "# Colonnes numériques (meta + stats) à inclure dans le modèle\n",
    "meta_cols = [\n",
    "    # Features produit\n",
    "    \"has_description\",\n",
    "    \"desc_len_tokens_raw\",\n",
    "    \"desc_is_long\",\n",
    "    \"title_has_digit\",\n",
    "    \n",
    "    # Statistiques de base sur le titre\n",
    "    \"designation_cleaned_len_char\",\n",
    "    \"designation_cleaned_len_tokens\",\n",
    "    \"designation_cleaned_ratio_digits\",\n",
    "    \"designation_cleaned_ratio_upper\",\n",
    "    \"designation_cleaned_num_punct\",\n",
    "    \"designation_cleaned_num_size_patterns\",\n",
    "    \n",
    "    # Statistiques de base sur la description\n",
    "    \"description_cleaned_len_char\",\n",
    "    \"description_cleaned_len_tokens\",\n",
    "    \"description_cleaned_ratio_digits\",\n",
    "    \"description_cleaned_ratio_upper\",\n",
    "    \"description_cleaned_num_punct\",\n",
    "    \"description_cleaned_num_size_patterns\",\n",
    "    \n",
    "    # Features pragmatiques\n",
    "    \"num_exclamation\",\n",
    "    \"num_question\",\n",
    "    \"num_plus\",\n",
    "    \"num_slash\",\n",
    "    \"num_material_words\",\n",
    "    \"num_color_words\",\n",
    "    \n",
    "    # Features de duplication (issues du notebook 1)\n",
    "    \"dup_count\",\n",
    "    \"is_duplicated_group\",\n",
    "]\n",
    "\n",
    "# On s'assure que toutes les colonnes existent\n",
    "meta_cols = [c for c in meta_cols if c in df.columns]\n",
    "\n",
    "print(\"Nombre de colonnes numériques sélectionnées :\", len(meta_cols))\n",
    "\n",
    "# Variable cible\n",
    "y = df[\"prdtypecode\"].values\n",
    "\n",
    "# DataFrame de travail pour le ColumnTransformer\n",
    "X = df[text_cols + meta_cols]\n",
    "\n",
    "print(\"Forme de X :\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7ae46e",
   "metadata": {},
   "source": [
    "# ============================================\n",
    "# 5. Construction du ColumnTransformer + Pipeline\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc036d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('preprocess',\n",
      "                 ColumnTransformer(transformers=[('title_tfidf',\n",
      "                                                  TfidfVectorizer(lowercase=False,\n",
      "                                                                  max_df=0.8,\n",
      "                                                                  max_features=20000,\n",
      "                                                                  min_df=5,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               2),\n",
      "                                                                  tokenizer=<method 'split' of 'str' objects>),\n",
      "                                                  'designation_cleaned'),\n",
      "                                                 ('desc_tfidf',\n",
      "                                                  TfidfVectorizer(lowercase=False,\n",
      "                                                                  max_df=0.8,\n",
      "                                                                  max_features=30000,\n",
      "                                                                  min_df=5,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               2),\n",
      "                                                                  tokenizer=<method...\n",
      "                                                   'description_cleaned_ratio_digits',\n",
      "                                                   'description_cleaned_ratio_upper',\n",
      "                                                   'description_cleaned_num_punct',\n",
      "                                                   'description_cleaned_num_size_patterns',\n",
      "                                                   'num_exclamation',\n",
      "                                                   'num_question', 'num_plus',\n",
      "                                                   'num_slash',\n",
      "                                                   'num_material_words',\n",
      "                                                   'num_color_words',\n",
      "                                                   'dup_count',\n",
      "                                                   'is_duplicated_group'])])),\n",
      "                ('model',\n",
      "                 LogisticRegression(class_weight='balanced', max_iter=1000,\n",
      "                                    n_jobs=-1, solver='saga'))])\n"
     ]
    }
   ],
   "source": [
    "# Vectoriseur TF-IDF pour le titre\n",
    "tfidf_title = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.8,\n",
    "    lowercase=False,\n",
    "    tokenizer=str.split,  # Le texte est déjà nettoyé et séparé par espaces\n",
    ")\n",
    "\n",
    "# Vectoriseur TF-IDF pour la description\n",
    "tfidf_desc = TfidfVectorizer(\n",
    "    max_features=30000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.8,\n",
    "    lowercase=False,\n",
    "    tokenizer=str.split,\n",
    ")\n",
    "\n",
    "# Standardisation des features numériques\n",
    "num_scaler = StandardScaler(with_mean=False)  # with_mean=False pour compatibilité avec les matrices creuses\n",
    "\n",
    "# ColumnTransformer qui combine texte + numérique\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"title_tfidf\", tfidf_title, \"designation_cleaned\"),\n",
    "        (\"desc_tfidf\", tfidf_desc, \"description_cleaned\"),\n",
    "        (\"numeric\", num_scaler, meta_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",        # On ne garde que ce qui est spécifié\n",
    "    sparse_threshold=0.3,    # On garde une sortie creuse si possible\n",
    ")\n",
    "\n",
    "# Modèle linéaire pour la classification multi-classe\n",
    "log_reg = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",  # Important pour les classes déséquilibrées\n",
    "    solver=\"saga\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Pipeline complet : prétraitement + modèle\n",
    "clf_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", log_reg),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(clf_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fa353c",
   "metadata": {},
   "source": [
    "# ============================================\n",
    "# 6. Split entraînement / validation et apprentissage\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d77696a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille X_train : (67932, 26)\n",
      "Taille X_valid : (16984, 26)\n",
      "Entraînement du modèle (pipeline complet)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted F1 (validation) : 0.7417\n",
      "\n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          10       0.32      0.63      0.42       623\n",
      "          40       0.80      0.50      0.62       502\n",
      "          50       0.76      0.72      0.74       336\n",
      "          60       0.91      0.73      0.81       166\n",
      "        1140       0.71      0.76      0.74       534\n",
      "        1160       0.70      0.89      0.79       791\n",
      "        1180       0.50      0.58      0.54       153\n",
      "        1280       0.75      0.48      0.58       974\n",
      "        1281       0.52      0.56      0.54       414\n",
      "        1300       0.83      0.84      0.83      1009\n",
      "        1301       0.80      0.90      0.85       161\n",
      "        1302       0.75      0.66      0.71       498\n",
      "        1320       0.74      0.61      0.67       648\n",
      "        1560       0.80      0.72      0.76      1015\n",
      "        1920       0.89      0.85      0.87       861\n",
      "        1940       0.49      0.90      0.63       161\n",
      "        2060       0.77      0.72      0.75       999\n",
      "        2220       0.76      0.81      0.78       165\n",
      "        2280       0.81      0.80      0.80       952\n",
      "        2403       0.67      0.66      0.67       955\n",
      "        2462       0.66      0.81      0.72       284\n",
      "        2522       0.88      0.76      0.82       998\n",
      "        2582       0.66      0.70      0.68       518\n",
      "        2583       0.96      0.86      0.91      2042\n",
      "        2585       0.69      0.72      0.70       499\n",
      "        2705       0.58      0.72      0.64       552\n",
      "        2905       0.98      0.99      0.98       174\n",
      "\n",
      "    accuracy                           0.74     16984\n",
      "   macro avg       0.73      0.74      0.72     16984\n",
      "weighted avg       0.76      0.74      0.74     16984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "print(\"Taille X_train :\", X_train.shape)\n",
    "print(\"Taille X_valid :\", X_valid.shape)\n",
    "\n",
    "# Entraînement du pipeline complet\n",
    "print(\"Entraînement du modèle (pipeline complet)...\")\n",
    "clf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur le jeu de validation\n",
    "y_pred = clf_pipeline.predict(X_valid)\n",
    "\n",
    "# Évaluation\n",
    "weighted_f1 = f1_score(y_valid, y_pred, average=\"weighted\")\n",
    "print(f\"Weighted F1 (validation) : {weighted_f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report :\")\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274eb892",
   "metadata": {},
   "source": [
    "# ============================================\n",
    "# 7. (Optionnel) GridSearchCV sur quelques hyperparamètres\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc27cf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement de la GridSearch (peut être long)...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=0.5, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=10000; total time=10.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=0.5, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=10000; total time=10.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=0.5, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=10000; total time=10.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=0.5, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=20000; total time=10.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=1.0, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=10000; total time=11.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=1.0, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=10000; total time=11.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=1.0, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=10000; total time=11.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=0.5, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=20000; total time=11.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=0.5, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=20000; total time=11.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=1.0, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=20000; total time=11.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=0.5, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=10000; total time=11.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=0.5, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=20000; total time=12.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=0.5, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=10000; total time=12.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=0.5, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=20000; total time=12.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=0.5, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=10000; total time=12.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=0.5, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=20000; total time=13.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=2.0, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=10000; total time=10.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=1.0, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=20000; total time=11.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=2.0, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=10000; total time=10.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=1.0, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=20000; total time=12.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=2.0, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=10000; total time=11.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=1.0, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=10000; total time=12.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=2.0, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=20000; total time=11.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=1.0, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=10000; total time=12.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=2.0, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=20000; total time=10.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=1.0, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=20000; total time=12.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=1.0, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=10000; total time=13.1min\n",
      "[CV] END model__C=1.0, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=20000; total time=12.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=2.0, preprocess__desc_tfidf__max_features=20000, preprocess__title_tfidf__max_features=20000; total time=11.4min\n",
      "[CV] END model__C=1.0, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=20000; total time=12.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=2.0, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=10000; total time=11.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=2.0, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=10000; total time=11.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=2.0, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=20000; total time= 5.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=2.0, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=20000; total time= 4.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=2.0, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=10000; total time= 5.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END model__C=2.0, preprocess__desc_tfidf__max_features=30000, preprocess__title_tfidf__max_features=20000; total time= 4.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres trouvés : {'model__C': 2.0, 'preprocess__desc_tfidf__max_features': 20000, 'preprocess__title_tfidf__max_features': 10000}\n",
      "Meilleur score (F1 pondéré, CV) : 0.7515288981620865\n",
      "\n",
      "Weighted F1 du meilleur modèle sur validation : 0.7501\n",
      "\n",
      "Classification report du meilleur modèle :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          10       0.32      0.64      0.43       623\n",
      "          40       0.79      0.52      0.63       502\n",
      "          50       0.78      0.75      0.77       336\n",
      "          60       0.89      0.73      0.81       166\n",
      "        1140       0.72      0.77      0.74       534\n",
      "        1160       0.74      0.89      0.81       791\n",
      "        1180       0.49      0.58      0.53       153\n",
      "        1280       0.75      0.49      0.59       974\n",
      "        1281       0.53      0.56      0.55       414\n",
      "        1300       0.84      0.86      0.85      1009\n",
      "        1301       0.81      0.91      0.86       161\n",
      "        1302       0.76      0.67      0.71       498\n",
      "        1320       0.73      0.63      0.67       648\n",
      "        1560       0.81      0.72      0.77      1015\n",
      "        1920       0.89      0.85      0.87       861\n",
      "        1940       0.51      0.90      0.65       161\n",
      "        2060       0.77      0.73      0.75       999\n",
      "        2220       0.75      0.81      0.78       165\n",
      "        2280       0.83      0.80      0.81       952\n",
      "        2403       0.68      0.67      0.67       955\n",
      "        2462       0.67      0.81      0.73       284\n",
      "        2522       0.89      0.78      0.83       998\n",
      "        2582       0.65      0.70      0.68       518\n",
      "        2583       0.97      0.87      0.91      2042\n",
      "        2585       0.69      0.73      0.71       499\n",
      "        2705       0.59      0.72      0.65       552\n",
      "        2905       0.98      0.99      0.98       174\n",
      "\n",
      "    accuracy                           0.74     16984\n",
      "   macro avg       0.73      0.74      0.73     16984\n",
      "weighted avg       0.77      0.74      0.75     16984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Petite grille de paramètres pour démonstration\n",
    "param_grid = {\n",
    "    \"model__C\": [0.5, 1.0, 2.0],\n",
    "    \"preprocess__title_tfidf__max_features\": [10000, 20000],\n",
    "    \"preprocess__desc_tfidf__max_features\": [20000, 30000],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=clf_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1_weighted\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "print(\"Lancement de la GridSearch (peut être long)...\")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Meilleurs paramètres trouvés :\", grid.best_params_)\n",
    "print(\"Meilleur score (F1 pondéré, CV) :\", grid.best_score_)\n",
    "\n",
    "# Évaluation du meilleur modèle sur le jeu de validation\n",
    "best_model = grid.best_estimator_\n",
    "y_pred_best = best_model.predict(X_valid)\n",
    "best_f1 = f1_score(y_valid, y_pred_best, average=\"weighted\")\n",
    "print(f\"\\nWeighted F1 du meilleur modèle sur validation : {best_f1:.4f}\")\n",
    "print(\"\\nClassification report du meilleur modèle :\")\n",
    "print(classification_report(y_valid, y_pred_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb9b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
