{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT + ResNet50 Fusion Model (FIXED VERSION)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Setup & Install\n",
    "!pip install -q wandb transformers gdown scikit-learn\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gdown\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from transformers import ViTForImageClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 2. Download Data with PROPER SPLIT\n",
    "def load_csv_from_gdrive(share_url: str, **read_csv_kwargs) -> pd.DataFrame:\n",
    "    file_id = share_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    return pd.read_csv(download_url, **read_csv_kwargs)\n",
    "\n",
    "print(\"Downloading CSV data...\")\n",
    "X_train_url = \"https://drive.google.com/file/d/1geSiJTTjamysiSbJ8-W9gR1kv-x6HyEd/view?usp=drive_link\"\n",
    "y_train_url = \"https://drive.google.com/file/d/16czWmLR5Ff0s5aYIqy1rHT7hc6Gcpfw3/view?usp=sharing\"\n",
    "\n",
    "X_train_full = load_csv_from_gdrive(X_train_url)\n",
    "y_train_full = load_csv_from_gdrive(y_train_url)\n",
    "\n",
    "print(f\"Total data loaded: {len(X_train_full):,} samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# ✅  DATA SPLIT (85% dev / 15% holdout)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPLITTING DATA (85% dev / 15% holdout)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_dev, X_holdout, y_dev, y_holdout = train_test_split(\n",
    "    X_train_full,\n",
    "    y_train_full['prdtypecode'],\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=y_train_full['prdtypecode']\n",
    ")\n",
    "\n",
    "df_dev = X_dev.copy()\n",
    "df_dev['prdtypecode'] = y_dev\n",
    "\n",
    "df_holdout = X_holdout.copy()\n",
    "df_holdout['prdtypecode'] = y_holdout\n",
    "\n",
    "print(f\"✓ Development set: {len(df_dev):,} samples (85%)\")\n",
    "print(f\"✓ Hold-out test set: {len(df_holdout):,} samples (15%)\")\n",
    "print(f\"✓ Classes: {df_dev['prdtypecode'].nunique()}\")\n",
    "print(\"\\n⚠️  CRITICAL: Holdout set will ONLY be used for final evaluation!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 3. Download Images\n",
    "IMAGE_FILE_ID = \"15ZkS0iTQ7j3mHpxil4mABlXwP-jAN_zi\"\n",
    "\n",
    "if not os.path.exists(\"/content/images\"):\n",
    "    print(\"Downloading images...\")\n",
    "    !mkdir -p /content/tmp /content/images\n",
    "    !gdown --id $IMAGE_FILE_ID -O /content/tmp/images.zip\n",
    "    print(\"Unzipping images...\")\n",
    "    !unzip -q -o /content/tmp/images.zip -d /content/images\n",
    "    print(\"Images unzipped\")\n",
    "else:\n",
    "    print(\"Images already exist\")\n",
    "\n",
    "IMG_ROOT = \"/content/images/images/image_train\"\n",
    "print(f\"Image root: {IMG_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 4. Download Pretrained Models\n",
    "def download_model_from_drive(url, output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        file_id = url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "        gdown_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "        gdown.download(gdown_url, output_path, quiet=False)\n",
    "    else:\n",
    "        print(f\"{output_path} already exists\")\n",
    "\n",
    "resnet_url = \"https://drive.google.com/file/d/1MWtAKTHtx_1qRYLEac-DDAStZdFD47yq/view?usp=sharing\"\n",
    "vit_url = \"https://drive.google.com/file/d/1WDaQZgNKuLvPq_J4HdWtRk1bSECPzip8/view?usp=sharing\"\n",
    "\n",
    "RESNET_PATH = '/content/best_model_resnet.pth'\n",
    "VIT_PATH = '/content/best_model_vit.pth'\n",
    "\n",
    "print(\"Downloading model weights...\")\n",
    "download_model_from_drive(resnet_url, RESNET_PATH)\n",
    "download_model_from_drive(vit_url, VIT_PATH)\n",
    "print(\"Models ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 5. Label Encoding (FIT ON DEV ONLY!)\n",
    "print(\"=\"*80)\n",
    "print(\"LABEL ENCODING (DEV SET ONLY)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ✅ FIX: Fit LabelEncoder ONLY on dev set (no data leakage)\n",
    "le = LabelEncoder()\n",
    "le.fit(df_dev['prdtypecode'])\n",
    "\n",
    "df_dev['encoded_label'] = le.transform(df_dev['prdtypecode'])\n",
    "df_holdout['encoded_label'] = le.transform(df_holdout['prdtypecode'])\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "print(f\"✓ LabelEncoder fitted on dev set ONLY (no data leakage)\")\n",
    "print(f\"✓ Number of classes: {num_classes}\")\n",
    "assert num_classes == 27, f\"Expected 27 classes, got {num_classes}\"\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 6. Split Dev into Train/Val\n",
    "CONFIG = {\n",
    "    \"random_state\": 42,\n",
    "    \"val_split\": 0.15,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 2,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 1e-3,\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SPLITTING DEV SET (85% train / 15% val)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_indices, val_indices = train_test_split(\n",
    "    df_dev.index,\n",
    "    test_size=CONFIG['val_split'],\n",
    "    random_state=CONFIG['random_state'],\n",
    "    stratify=df_dev['encoded_label']\n",
    ")\n",
    "\n",
    "df_train = df_dev.loc[train_indices].reset_index(drop=True)\n",
    "df_val = df_dev.loc[val_indices].reset_index(drop=True)\n",
    "\n",
    "total_samples = len(df_dev) + len(df_holdout)\n",
    "print(f\"✓ Training:   {len(df_train):,} samples (~{len(df_train)/total_samples*100:.1f}%)\")\n",
    "print(f\"✓ Validation: {len(df_val):,} samples (~{len(df_val)/total_samples*100:.1f}%)\")\n",
    "print(f\"✓ Hold-out:   {len(df_holdout):,} samples (15.0%)\")\n",
    "print(\"\\n⚠️  Model selection will use Train/Val ONLY\")\n",
    "print(\"⚠️  Holdout will be evaluated at the END\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 7. Define Models and Datasets\n",
    "class RakutenImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None, label_col=\"encoded_label\"):\n",
    "        self.image_ids = dataframe['imageid'].values\n",
    "        self.product_ids = dataframe['productid'].values\n",
    "        self.labels = dataframe[label_col].values\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        product_id = self.product_ids[idx]\n",
    "        label = self.labels[idx]\n",
    "        img_name = f\"image_{image_id}_product_{product_id}.jpg\"\n",
    "        img_path = self.image_dir / img_name\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except (FileNotFoundError, OSError):\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class ResNet50Classifier(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.3):\n",
    "        super(ResNet50Classifier, self).__init__()\n",
    "        self.backbone = models.resnet50(weights=None)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.custom_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        logits = self.custom_head(features)\n",
    "        return logits\n",
    "\n",
    "# Transforms\n",
    "resnet_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "vit_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load pretrained models\n",
    "print(\"Loading models...\")\n",
    "resnet_model = ResNet50Classifier(num_classes=num_classes)\n",
    "state_dict = torch.load(RESNET_PATH, map_location=CONFIG['device'])\n",
    "if 'model_state_dict' in state_dict:\n",
    "    state_dict = state_dict['model_state_dict']\n",
    "resnet_model.load_state_dict(state_dict)\n",
    "resnet_model.to(CONFIG['device'])\n",
    "resnet_model.eval()\n",
    "\n",
    "vit_model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    num_labels=num_classes,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "state_dict_vit = torch.load(VIT_PATH, map_location=CONFIG['device'])\n",
    "if 'model_state_dict' in state_dict_vit:\n",
    "    state_dict_vit = state_dict_vit['model_state_dict']\n",
    "vit_model.load_state_dict(state_dict_vit)\n",
    "vit_model.to(CONFIG['device'])\n",
    "vit_model.eval()\n",
    "\n",
    "print(\"✓ Models loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 8. Define Fusion Model\n",
    "class DualDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, trans_res, trans_vit, label_col='encoded_label'):\n",
    "        self.image_ids = df['imageid'].values\n",
    "        self.product_ids = df['productid'].values\n",
    "        self.labels = df[label_col].values\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.trans_res = trans_res\n",
    "        self.trans_vit = trans_vit\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        product_id = self.product_ids[idx]\n",
    "        label = self.labels[idx]\n",
    "        img_name = f\"image_{image_id}_product_{product_id}.jpg\"\n",
    "        img_path = self.img_dir / img_name\n",
    "\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            img = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "\n",
    "        return self.trans_res(img), self.trans_vit(img), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class FusionNet(nn.Module):\n",
    "    def __init__(self, resnet, vit, num_classes=27):\n",
    "        super().__init__()\n",
    "        self.resnet_backbone = resnet.backbone\n",
    "        self.vit_backbone = vit.vit\n",
    "\n",
    "        # Freeze backbones\n",
    "        for p in self.resnet_backbone.parameters(): \n",
    "            p.requires_grad = False\n",
    "        for p in self.vit_backbone.parameters(): \n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Fusion head (only trainable part)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(2816, 1024),  # 2048 (ResNet) + 768 (ViT)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_res, x_vit):\n",
    "        with torch.no_grad():\n",
    "            feat_res = self.resnet_backbone(x_res)\n",
    "            if len(feat_res.shape) > 2:\n",
    "                feat_res = torch.flatten(feat_res, 1)\n",
    "\n",
    "            out_vit = self.vit_backbone(pixel_values=x_vit)\n",
    "            feat_vit = out_vit.last_hidden_state[:, 0]\n",
    "\n",
    "        combined = torch.cat([feat_res, feat_vit], dim=1)\n",
    "        return self.head(combined)\n",
    "\n",
    "# Create datasets\n",
    "train_ds_dual = DualDataset(df_train, IMG_ROOT, resnet_transform, vit_transform)\n",
    "train_loader = DataLoader(train_ds_dual, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=CONFIG['num_workers'])\n",
    "\n",
    "val_ds_dual = DualDataset(df_val, IMG_ROOT, resnet_transform, vit_transform)\n",
    "val_loader = DataLoader(val_ds_dual, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=CONFIG['num_workers'])\n",
    "\n",
    "# Create holdout loader for final evaluation\n",
    "holdout_ds_dual = DualDataset(df_holdout, IMG_ROOT, resnet_transform, vit_transform)\n",
    "holdout_loader = DataLoader(holdout_ds_dual, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=CONFIG['num_workers'])\n",
    "\n",
    "# Initialize fusion model\n",
    "fusion_model = FusionNet(resnet_model, vit_model, num_classes=num_classes).to(CONFIG['device'])\n",
    "optimizer = AdamW(fusion_model.head.parameters(), lr=CONFIG['lr'], weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"✓ Fusion model initialized\")\n",
    "print(f\"  Trainable params: {sum(p.numel() for p in fusion_model.head.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 9. Training Loop with WandB\n",
    "import datetime\n",
    "\n",
    "# Detect execution environment\n",
    "import sys\n",
    "ENVIRONMENT = \"colab\" if 'google.colab' in sys.modules else \"local\"\n",
    "\n",
    "\n",
    "# Initialize WandB\n",
    "\n",
    "wandb.init(\n",
    "    project=\"rakuten-fusion\",\n",
    "    entity=\"xiaosong-dev-formation-data-science\",\n",
    "    name=f\"vit_resnet50_fusion_v1_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "    tags=[\"fusion\", \"vit\", \"v1\", \"production\", ENVIRONMENT],\n",
    "    config=CONFIG,\n",
    "    notes=\"ViT + ResNet50 multimodal fusion with proper data split\"\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING FUSION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "history = {\"train_loss\": [], \"val_f1\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    # Training\n",
    "    fusion_model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "    for x_res, x_vit, y in pbar:\n",
    "        x_res = x_res.to(CONFIG['device'])\n",
    "        x_vit = x_vit.to(CONFIG['device'])\n",
    "        y = y.to(CONFIG['device'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = fusion_model(x_res, x_vit)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    fusion_model.eval()\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_res, x_vit, y in val_loader:\n",
    "            x_res = x_res.to(CONFIG['device'])\n",
    "            x_vit = x_vit.to(CONFIG['device'])\n",
    "            outputs = fusion_model(x_res, x_vit)\n",
    "            val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            val_targets.extend(y.numpy())\n",
    "\n",
    "    val_f1 = f1_score(val_targets, val_preds, average='weighted')\n",
    "    val_acc = accuracy_score(val_targets, val_preds)\n",
    "\n",
    "    # Log to WandB\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"val_f1\": val_f1,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "    })\n",
    "\n",
    "    # Save history\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    history[\"val_f1\"].append(val_f1)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss={avg_train_loss:.4f}, Val F1={val_f1:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_f1)\n",
    "\n",
    "    # Save best model\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(fusion_model.state_dict(), \"fusion_model_best_FIXED.pth\")\n",
    "        print(f\"  ✅ Best model saved! (Val F1: {val_f1:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Training Complete. Best Val F1: {best_val_f1:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 10. FINAL EVALUATION ON HOLDOUT TEST SET\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION ON HOLDOUT TEST SET\")\n",
    "print(\"=\"*80)\n",
    "print(\"⚠️  This is the FIRST and ONLY time holdout data is used!\\n\")\n",
    "\n",
    "# Load best model\n",
    "fusion_model.load_state_dict(torch.load(\"fusion_model_best_FIXED.pth\"))\n",
    "fusion_model.eval()\n",
    "\n",
    "# Evaluate on holdout\n",
    "holdout_preds = []\n",
    "holdout_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_res, x_vit, y in tqdm(holdout_loader, desc=\"Holdout Evaluation\"):\n",
    "        x_res = x_res.to(CONFIG['device'])\n",
    "        x_vit = x_vit.to(CONFIG['device'])\n",
    "        outputs = fusion_model(x_res, x_vit)\n",
    "        holdout_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "        holdout_targets.extend(y.numpy())\n",
    "\n",
    "holdout_f1 = f1_score(holdout_targets, holdout_preds, average='weighted')\n",
    "holdout_acc = accuracy_score(holdout_targets, holdout_preds)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best Validation F1: {best_val_f1:.4f}\")\n",
    "print(f\"Holdout Test F1:    {holdout_f1:.4f}\")\n",
    "print(f\"Holdout Test Acc:   {holdout_acc:.4f}\")\n",
    "print(f\"Difference:         {holdout_f1 - best_val_f1:+.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Log final results to WandB\n",
    "wandb.log({\n",
    "    \"final/best_val_f1\": best_val_f1,\n",
    "    \"final/holdout_f1\": holdout_f1,\n",
    "    \"final/holdout_acc\": holdout_acc\n",
    "})\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report (Holdout):\")\n",
    "print(classification_report(holdout_targets, holdout_preds, digits=4, zero_division=0))\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 11. Save to Google Drive (Optional)\n",
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "target_dir = \"/content/drive/MyDrive/Rakuten_models\"\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "target_file = os.path.join(target_dir, f\"fusion_vit_resnet_FIXED_{timestamp}.pth\")\n",
    "shutil.copy(\"fusion_model_best_FIXED.pth\", target_file)\n",
    "print(f\"✓ Model saved to: {target_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}