{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rakuten Image Classification - Production ViT Training\n",
    "\n",
    "**Objectif:** Entra√Æner le mod√®le Vision Transformer (google/vit-base-patch16-224) sur l'ensemble complet des donn√©es.\n",
    "\n",
    "**Mat√©riel:** NVIDIA RTX 3060 Ti (8GB VRAM)\n",
    "\n",
    "**Strat√©gie:**\n",
    "- Transfer learning avec ViT pr√©-entra√Æn√©\n",
    "- Optimiseur AdamW avec warmup scheduler\n",
    "- Early stopping bas√© sur la validation accuracy\n",
    "- Monitoring en temps r√©el avec graphiques\n",
    "\n",
    "**R√©sultats attendus:**\n",
    "- Temps d'entra√Ænement: ~3-4 heures pour 20 epochs\n",
    "- Val accuracy: ~50-60%\n",
    "- Checkpoints sauvegard√©s dans: `/workspace/checkpoints/vit_production/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Imports standards\nimport sys\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport numpy as np\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\nfrom tqdm import tqdm\nimport json\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, clear_output\nfrom torch.cuda.amp import GradScaler\nimport wandb\n\n# Imports transformers\nfrom transformers import ViTForImageClassification, get_scheduler\nfrom torch.optim import AdamW\n\n# Add project root and scripts to path\nproject_root = Path.cwd().parent.parent\nscripts_dir = project_root / \"scripts\"\nif str(project_root) not in sys.path:\n    sys.path.insert(0, str(project_root))\nif str(scripts_dir) not in sys.path:\n    sys.path.insert(0, str(scripts_dir))\n\n# Import rakuten modules\nfrom src.rakuten_image.datasets import RakutenImageDataset\nfrom load_data import split_data\n\nprint(\"‚úì All modules imported successfully\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\nwandb.login()"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üöÄ CONFIGURATION ViT PRODUCTION\n",
      "================================================================================\n",
      "Device: cuda\n",
      "Mod√®le: google/vit-base-patch16-224\n",
      "Taille d'image: 224x224\n",
      "Batch size: 32\n",
      "Epochs max: 20\n",
      "Learning rate: 2e-05\n",
      "Warmup ratio: 0.1\n",
      "Val split: 0.15 (15%)\n",
      "Early stopping patience: 3\n",
      "AMP activ√©: True\n",
      "R√©pertoire checkpoints: /workspace/checkpoints/vit_production\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION D'ENTRA√éNEMENT\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Chemins des donn√©es\n",
    "    \"data_dir\": Path(\"/workspace/data\"),\n",
    "    \"img_dir\": Path(\"/workspace/data/images/image_train\"),\n",
    "    \"checkpoint_dir\": Path(\"/workspace/checkpoints/vit_production\"),\n",
    "\n",
    "    # Configuration du mod√®le\n",
    "    \"model_name\": \"google/vit-base-patch16-224\",\n",
    "    \"img_size\": 224,\n",
    "\n",
    "    # Hyperparam√®tres d'entra√Ænement\n",
    "    \"batch_size\": 32,  # Augment√© √† 32 avec AMP pour RTX 3060 Ti 8GB\n",
    "    \"num_epochs\": 20,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"weight_decay\": 0.05,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "\n",
    "    # Split train/validation (85% train, 15% validation)\n",
    "    \"val_split\": 0.15,\n",
    "    \"random_state\": 42,\n",
    "\n",
    "    # Early stopping\n",
    "    \"early_stopping_patience\": 3,\n",
    "\n",
    "    # Configuration hardware\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"num_workers\": 2,\n",
    "    \"use_amp\": True,  # Utiliser Automatic Mixed Precision\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ CONFIGURATION ViT PRODUCTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Mod√®le: {CONFIG['model_name']}\")\n",
    "print(f\"Taille d'image: {CONFIG['img_size']}x{CONFIG['img_size']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Epochs max: {CONFIG['num_epochs']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Warmup ratio: {CONFIG['warmup_ratio']}\")\n",
    "print(f\"Val split: {CONFIG['val_split']} ({int(CONFIG['val_split']*100)}%)\")\n",
    "print(f\"Early stopping patience: {CONFIG['early_stopping_patience']}\")\n",
    "print(f\"AMP activ√©: {CONFIG['use_amp']}\")\n",
    "print(f\"R√©pertoire checkpoints: {CONFIG['checkpoint_dir']}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et Pr√©paration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Loading data...\n",
      "‚úì Data loaded: 84,916 total samples\n",
      "  Development: 72,178 samples (85%)\n",
      "  Hold-out:    12,738 samples (15%)\n",
      "  Unique classes: 27\n",
      "\n",
      "üîß Label encoding...\n",
      "‚úì Encoding complete\n",
      "  Encoded range: 0 to 26\n",
      "  Total classes: 27\n",
      "\n",
      "‚ö†Ô∏è  IMPORTANT: Using SAME split as text notebooks!\n",
      "‚ö†Ô∏è  This ensures consistent evaluation across modalities\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìÇ Loading data...\")\n",
    "\n",
    "# Use unified split function (SAME as text notebooks!)\n",
    "X_dev, X_holdout, y_dev, y_holdout = split_data()\n",
    "\n",
    "# Create full dataframes\n",
    "df_dev = X_dev.copy()\n",
    "df_dev['prdtypecode'] = y_dev\n",
    "\n",
    "df_holdout = X_holdout.copy()\n",
    "df_holdout['prdtypecode'] = y_holdout\n",
    "\n",
    "print(f\"‚úì Data loaded: {len(df_dev) + len(df_holdout):,} total samples\")\n",
    "print(f\"  Development: {len(df_dev):,} samples (85%)\")\n",
    "print(f\"  Hold-out:    {len(df_holdout):,} samples (15%)\")\n",
    "print(f\"  Unique classes: {df_dev['prdtypecode'].nunique()}\")\n",
    "\n",
    "# Global Label Encoding\n",
    "print(\"\\nüîß Label encoding...\")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit on combined data to ensure all classes are seen\n",
    "all_labels = pd.concat([df_dev['prdtypecode'], df_holdout['prdtypecode']])\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "df_dev['encoded_label'] = label_encoder.transform(df_dev['prdtypecode'])\n",
    "df_holdout['encoded_label'] = label_encoder.transform(df_holdout['prdtypecode'])\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(f\"‚úì Encoding complete\")\n",
    "print(f\"  Encoded range: 0 to {num_classes - 1}\")\n",
    "print(f\"  Total classes: {num_classes}\")\n",
    "assert num_classes == 27, f\"Expected 27 classes, got {num_classes}\"\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  IMPORTANT: Using SAME split as text notebooks!\")\n",
    "print(\"‚ö†Ô∏è  This ensures consistent evaluation across modalities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Data Cleaning & Train/Val Split\n",
      "================================================================================\n",
      "\n",
      "üîß Data cleaning (development set)...\n",
      "‚úì Development set after cleaning: 72,178 samples\n",
      "\n",
      "üîß Data cleaning (hold-out set)...\n",
      "‚úì Hold-out set after cleaning: 12,738 samples\n",
      "\n",
      "================================================================================\n",
      "Development Split (Train/Val for Hyperparameter Tuning)\n",
      "================================================================================\n",
      "‚úì Development split complete:\n",
      "  Training:   61,351 samples (~72.2%)\n",
      "  Validation: 10,827 samples (~12.8%)\n",
      "  Hold-out:   12,738 samples (15.0%)\n",
      "\n",
      "================================================================================\n",
      "DATA SPLIT SUMMARY\n",
      "================================================================================\n",
      "Total: 84,916 samples\n",
      "  1. Training:   61,351 (for model training)\n",
      "  2. Validation: 10,827 (for hyperparameter tuning)\n",
      "  3. Hold-out:   12,738 (for final evaluation)\n",
      "\n",
      "‚ö†Ô∏è  CRITICAL: This split is IDENTICAL to text notebooks!\n",
      "‚ö†Ô∏è  Image and text models evaluated on same hold-out samples\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Data Cleaning & Train/Val Split\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Data cleaning (development set)\n",
    "print(\"\\nüîß Data cleaning (development set)...\")\n",
    "original_dev_size = len(df_dev)\n",
    "\n",
    "missing_images_dev = []\n",
    "for idx, row in df_dev.iterrows():\n",
    "    imageid = int(row['imageid'])\n",
    "    productid = int(row['productid'])\n",
    "    img_path = CONFIG[\"img_dir\"] / f\"image_{imageid}_product_{productid}.jpg\"\n",
    "    if not img_path.exists():\n",
    "        missing_images_dev.append(idx)\n",
    "\n",
    "if missing_images_dev:\n",
    "    df_dev = df_dev.drop(missing_images_dev)\n",
    "    print(f\"  Removed {len(missing_images_dev)} samples with missing images\")\n",
    "\n",
    "print(f\"‚úì Development set after cleaning: {len(df_dev):,} samples\")\n",
    "\n",
    "# Data cleaning (hold-out set)\n",
    "print(\"\\nüîß Data cleaning (hold-out set)...\")\n",
    "original_holdout_size = len(df_holdout)\n",
    "\n",
    "missing_images_holdout = []\n",
    "for idx, row in df_holdout.iterrows():\n",
    "    imageid = int(row['imageid'])\n",
    "    productid = int(row['productid'])\n",
    "    img_path = CONFIG[\"img_dir\"] / f\"image_{imageid}_product_{productid}.jpg\"\n",
    "    if not img_path.exists():\n",
    "        missing_images_holdout.append(idx)\n",
    "\n",
    "if missing_images_holdout:\n",
    "    df_holdout = df_holdout.drop(missing_images_holdout)\n",
    "    print(f\"  Removed {len(missing_images_holdout)} samples with missing images\")\n",
    "\n",
    "print(f\"‚úì Hold-out set after cleaning: {len(df_holdout):,} samples\")\n",
    "\n",
    "# Split development set into train/val for hyperparameter tuning\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Development Split (Train/Val for Hyperparameter Tuning)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_indices, val_indices, _, _ = train_test_split(\n",
    "    df_dev.index,\n",
    "    df_dev['encoded_label'],\n",
    "    test_size=0.15,\n",
    "    random_state=CONFIG[\"random_state\"],\n",
    "    stratify=df_dev['encoded_label']\n",
    ")\n",
    "\n",
    "df_train = df_dev.loc[train_indices].reset_index(drop=True)\n",
    "df_val = df_dev.loc[val_indices].reset_index(drop=True)\n",
    "df_holdout = df_holdout.reset_index(drop=True)\n",
    "\n",
    "total_samples = len(df_dev) + len(df_holdout)\n",
    "print(f\"‚úì Development split complete:\")\n",
    "print(f\"  Training:   {len(df_train):,} samples (~{len(df_train)/total_samples*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(df_val):,} samples (~{len(df_val)/total_samples*100:.1f}%)\")\n",
    "print(f\"  Hold-out:   {len(df_holdout):,} samples (15.0%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA SPLIT SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total: {total_samples:,} samples\")\n",
    "print(f\"  1. Training:   {len(df_train):,} (for model training)\")\n",
    "print(f\"  2. Validation: {len(df_val):,} (for hyperparameter tuning)\")\n",
    "print(f\"  3. Hold-out:   {len(df_holdout):,} (for final evaluation)\")\n",
    "print()\n",
    "print(\"‚ö†Ô∏è  CRITICAL: This split is IDENTICAL to text notebooks!\")\n",
    "print(\"‚ö†Ô∏è  Image and text models evaluated on same hold-out samples\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Cr√©ation des datasets et dataloaders...\n",
      "Pre-loading paths into memory...\n",
      "Dataset initialized with 61351 samples.\n",
      "Pre-loading paths into memory...\n",
      "Dataset initialized with 10827 samples.\n",
      "Pre-loading paths into memory...\n",
      "Dataset initialized with 12738 samples.\n",
      "‚úì Datasets cr√©√©s\n",
      "  Training:   61,351 √©chantillons\n",
      "  Validation: 10,827 √©chantillons\n",
      "  Hold-out:   12,738 √©chantillons\n",
      "  Classes:    27\n",
      "\n",
      "‚úì DataLoaders cr√©√©s (batch_size=32)\n",
      "  Train:      1,917 batches\n",
      "  Validation: 339 batches\n",
      "  Hold-out:   399 batches\n",
      "\n",
      "‚úì Sanity check: Batch shape torch.Size([32, 3, 224, 224]), Labels torch.Size([32])\n",
      "‚úÖ All DataLoaders working correctly!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîß Cr√©ation des datasets et dataloaders...\")\n",
    "\n",
    "# Transformations pour l'entra√Ænement (avec augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG[\"img_size\"], CONFIG[\"img_size\"])),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Transformations pour validation/test (sans augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG[\"img_size\"], CONFIG[\"img_size\"])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Cr√©er les datasets (utilisant encoded_label au lieu de prdtypecode)\n",
    "train_dataset = RakutenImageDataset(\n",
    "    dataframe=df_train,\n",
    "    image_dir=CONFIG[\"img_dir\"],\n",
    "    transform=train_transform,\n",
    "    label_col=\"encoded_label\"\n",
    ")\n",
    "\n",
    "val_dataset = RakutenImageDataset(\n",
    "    dataframe=df_val,\n",
    "    image_dir=CONFIG[\"img_dir\"],\n",
    "    transform=val_transform,\n",
    "    label_col=\"encoded_label\"\n",
    ")\n",
    "\n",
    "test_dataset = RakutenImageDataset(\n",
    "    dataframe=df_holdout,\n",
    "    image_dir=CONFIG[\"img_dir\"],\n",
    "    transform=val_transform,\n",
    "    label_col=\"encoded_label\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Datasets cr√©√©s\")\n",
    "print(f\"  Training:   {len(train_dataset):,} √©chantillons\")\n",
    "print(f\"  Validation: {len(val_dataset):,} √©chantillons\")\n",
    "print(f\"  Hold-out:   {len(test_dataset):,} √©chantillons\")\n",
    "print(f\"  Classes:    {train_dataset.num_classes}\")\n",
    "\n",
    "# Cr√©er les dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    pin_memory=True,\n",
    "    drop_last=True  # Pour stabilit√© BatchNorm\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì DataLoaders cr√©√©s (batch_size={CONFIG['batch_size']})\")\n",
    "print(f\"  Train:      {len(train_loader):,} batches\")\n",
    "print(f\"  Validation: {len(val_loader):,} batches\")\n",
    "print(f\"  Hold-out:   {len(test_loader):,} batches\")\n",
    "\n",
    "# Sanity check\n",
    "try:\n",
    "    images, labels = next(iter(train_loader))\n",
    "    print(f\"\\n‚úì Sanity check: Batch shape {images.shape}, Labels {labels.shape}\")\n",
    "    print(f\"‚úÖ All DataLoaders working correctly!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialisation du Mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèóÔ∏è Initialisation du mod√®le ViT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1d7ebc97f24badbfc5306ab6bb9a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e9238140ec4e61a7a99b84fd535b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([27]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([27, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Mod√®le charg√©: google/vit-base-patch16-224\n",
      "  Nombre de classes: 27\n",
      "  Param√®tres totaux: 85,819,419\n",
      "  Param√®tres entra√Ænables: 85,819,419\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüèóÔ∏è Initialisation du mod√®le ViT...\")\n",
    "\n",
    "# Charger le mod√®le pr√©-entra√Æn√©\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    num_labels=num_classes,  # Utiliser num_classes de l'encodage global\n",
    "    ignore_mismatched_sizes=True,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1\n",
    ")\n",
    "model.to(CONFIG[\"device\"])\n",
    "\n",
    "print(f\"‚úì Mod√®le charg√©: {CONFIG['model_name']}\")\n",
    "print(f\"  Nombre de classes: {num_classes}\")\n",
    "\n",
    "# Compter les param√®tres\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"  Param√®tres totaux: {total_params:,}\")\n",
    "print(f\"  Param√®tres entra√Ænables: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è Configuration de l'optimiseur et du scheduler...\n",
      "‚úì Optimiseur: AdamW\n",
      "  Learning rate: 2e-05\n",
      "  Weight decay: 0.05\n",
      "‚úì Scheduler: Linear warmup\n",
      "  Warmup steps: 3,834/38,340\n",
      "‚úì GradScaler initialis√© pour AMP\n",
      "‚úì R√©pertoire checkpoints: /workspace/checkpoints/vit_production\n",
      "‚úì Configuration sauvegard√©e\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n‚öôÔ∏è Configuration de l'optimiseur et du scheduler...\")\n",
    "\n",
    "# Optimiseur AdamW\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "# Scheduler avec warmup lin√©aire\n",
    "total_steps = len(train_loader) * CONFIG[\"num_epochs\"]\n",
    "num_warmup_steps = int(total_steps * CONFIG[\"warmup_ratio\"])\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Initialiser GradScaler pour AMP\n",
    "scaler = torch.amp.GradScaler('cuda') if CONFIG[\"use_amp\"] else None\n",
    "\n",
    "print(f\"‚úì Optimiseur: AdamW\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Weight decay: {CONFIG['weight_decay']}\")\n",
    "print(f\"‚úì Scheduler: Linear warmup\")\n",
    "print(f\"  Warmup steps: {num_warmup_steps:,}/{total_steps:,}\")\n",
    "if CONFIG[\"use_amp\"]:\n",
    "    print(f\"‚úì GradScaler initialis√© pour AMP\")\n",
    "\n",
    "# Cr√©er le r√©pertoire de checkpoints\n",
    "CONFIG[\"checkpoint_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "print(f\"‚úì R√©pertoire checkpoints: {CONFIG['checkpoint_dir']}\")\n",
    "\n",
    "# Sauvegarder la configuration\n",
    "with open(CONFIG[\"checkpoint_dir\"] / \"config.json\", \"w\") as f:\n",
    "    json.dump({k: str(v) for k, v in CONFIG.items()}, f, indent=2)\n",
    "print(f\"‚úì Configuration sauvegard√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entra√Ænement avec Monitoring en Temps R√©el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"üöÄ D√âMARRAGE DE L'ENTRA√éNEMENT\")\nprint(\"=\" * 80)\nprint(f\"Temps estim√©: ~{len(train_loader) * CONFIG['num_epochs'] * 0.8 / 60:.0f} minutes\")\nprint(f\"AMP activ√©: {CONFIG['use_amp']}\")\nprint(\"=\" * 80 + \"\\n\")\n\n# Initialize WandB\nwandb.init(\n    project=\"rakuten-classification\",\n    entity=\"xiaosong-dev-formation-data-science\",\n    config=CONFIG,\n    name=f\"vit_run_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}\"\n)\n\n# Variables pour le tracking\nbest_val_acc = 0.0\nbest_val_loss = float('inf')\nbest_val_f1 = 0.0\npatience_counter = 0\nhistory = {\n    \"train_loss\": [],\n    \"train_acc\": [],\n    \"val_loss\": [],\n    \"val_acc\": [],\n    \"val_f1\": []\n}\n\n# Fonction pour mettre √† jour les graphiques\ndef update_plots():\n    clear_output(wait=True)\n    fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n    \n    # Plot Loss\n    axes[0].plot(history[\"train_loss\"], label='Train Loss', marker='o', linewidth=2)\n    axes[0].plot(history[\"val_loss\"], label='Val Loss', marker='s', linewidth=2)\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Training et Validation Loss')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    # Plot Accuracy\n    axes[1].plot(history[\"train_acc\"], label='Train Acc', marker='o', linewidth=2)\n    axes[1].plot(history[\"val_acc\"], label='Val Acc', marker='s', linewidth=2)\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Accuracy (%)')\n    axes[1].set_title('Training et Validation Accuracy')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Afficher les statistiques\n    print(f\"\\nüìä Statistiques actuelles:\")\n    print(f\"  Meilleure Val Acc: {best_val_acc:.2f}% | F1: {best_val_f1:.4f} (Patience: {patience_counter}/{CONFIG['early_stopping_patience']})\")\n    print(f\"  Meilleure Val Loss: {best_val_loss:.4f}\")\n\n# Boucle d'entra√Ænement principale\nfor epoch in range(CONFIG[\"num_epochs\"]):\n    print(f\"\\nEpoch {epoch + 1}/{CONFIG['num_epochs']}\")\n    print(\"=\" * 80)\n    \n    # -------------------- Phase d'entra√Ænement --------------------\n    model.train()\n    train_loss = 0.0\n    train_correct = 0\n    train_total = 0\n    \n    train_pbar = tqdm(train_loader, desc=f\"Training\", unit=\"batch\")\n    for images, labels in train_pbar:\n        images, labels = images.to(CONFIG[\"device\"]), labels.to(CONFIG[\"device\"])\n        \n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward pass avec AMP\n        if CONFIG[\"use_amp\"]:\n            with torch.amp.autocast(device_type=\"cuda\"):\n                outputs = model(pixel_values=images, labels=labels)\n                loss = outputs.loss\n            \n            # Backward pass avec scaler\n            scaler.scale(loss).backward()\n            \n            # Unscale avant gradient clipping\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            \n            # Optimizer step avec scaler\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            # Forward pass sans AMP\n            outputs = model(pixel_values=images, labels=labels)\n            loss = outputs.loss\n            \n            # Backward pass standard\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        \n        scheduler.step()\n        \n        # M√©triques\n        train_loss += loss.item()\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        train_correct += (predictions == labels).sum().item()\n        train_total += labels.size(0)\n        \n        # Mise √† jour de la barre de progression\n        train_pbar.set_postfix({\n            'loss': f'{loss.item():.4f}',\n            'acc': f'{100.0 * train_correct / train_total:.2f}%'\n        })\n    \n    avg_train_loss = train_loss / len(train_loader)\n    train_accuracy = 100.0 * train_correct / train_total\n    \n    # -------------------- Phase de validation --------------------\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    \n    # Collect all predictions and labels for F1 score\n    all_val_preds = []\n    all_val_labels = []\n    \n    val_pbar = tqdm(val_loader, desc=f\"Validation\", unit=\"batch\")\n    with torch.no_grad():\n        for images, labels in val_pbar:\n            images, labels = images.to(CONFIG[\"device\"]), labels.to(CONFIG[\"device\"])\n            \n            # Forward pass validation (avec AMP si activ√©)\n            if CONFIG[\"use_amp\"]:\n                with torch.amp.autocast(device_type=\"cuda\"):\n                    outputs = model(pixel_values=images, labels=labels)\n                    loss = outputs.loss\n            else:\n                outputs = model(pixel_values=images, labels=labels)\n                loss = outputs.loss\n            \n            val_loss += loss.item()\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            val_correct += (predictions == labels).sum().item()\n            val_total += labels.size(0)\n            \n            # Collect for F1 score\n            all_val_preds.extend(predictions.cpu().numpy())\n            all_val_labels.extend(labels.cpu().numpy())\n            \n            val_pbar.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'acc': f'{100.0 * val_correct / val_total:.2f}%'\n            })\n    \n    avg_val_loss = val_loss / len(val_loader)\n    val_accuracy = 100.0 * val_correct / val_total\n    \n    # Calculate F1 score (weighted average for multi-class)\n    val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n    \n    # -------------------- Mise √† jour de l'historique --------------------\n    history[\"train_loss\"].append(avg_train_loss)\n    history[\"train_acc\"].append(train_accuracy)\n    history[\"val_loss\"].append(avg_val_loss)\n    history[\"val_acc\"].append(val_accuracy)\n    history[\"val_f1\"].append(val_f1)\n    \n    # Log metrics to WandB (including F1 score)\n    wandb.log({\n        \"train_loss\": avg_train_loss,\n        \"train_acc\": train_accuracy,\n        \"val_loss\": avg_val_loss,\n        \"val_acc\": val_accuracy,\n        \"val_f1\": val_f1,\n        \"epoch\": epoch + 1,\n        \"learning_rate\": optimizer.param_groups[0]['lr']\n    })\n    \n    print(f\"\\nüìä R√©sultats Epoch {epoch + 1}:\")\n    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.2f}%\")\n    print(f\"  Val Loss:   {avg_val_loss:.4f} | Val Acc:   {val_accuracy:.2f}% | F1: {val_f1:.4f}\")\n    \n    # -------------------- Sauvegarde du meilleur mod√®le --------------------\n    if val_accuracy > best_val_acc:\n        best_val_acc = val_accuracy\n        best_val_loss = avg_val_loss\n        best_val_f1 = val_f1\n        patience_counter = 0\n        \n        checkpoint_path = CONFIG[\"checkpoint_dir\"] / \"best_model.pth\"\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'scaler_state_dict': scaler.state_dict() if CONFIG[\"use_amp\"] else None,\n            'val_acc': val_accuracy,\n            'val_loss': avg_val_loss,\n            'val_f1': val_f1,\n            'train_acc': train_accuracy,\n            'train_loss': avg_train_loss,\n        }, checkpoint_path)\n        \n        print(f\"  ‚úÖ Meilleur mod√®le sauvegard√©! (Val Acc: {val_accuracy:.2f}%, F1: {val_f1:.4f})\")\n    else:\n        patience_counter += 1\n        print(f\"  ‚è≥ Pas d'am√©lioration ({patience_counter}/{CONFIG['early_stopping_patience']})\")\n    \n    # -------------------- Mise √† jour des graphiques --------------------\n    update_plots()\n    \n    # -------------------- Early Stopping --------------------\n    if patience_counter >= CONFIG[\"early_stopping_patience\"]:\n        print(f\"\\n‚ö†Ô∏è Early stopping d√©clench√© apr√®s {epoch + 1} epochs\")\n        break\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ ENTRA√éNEMENT TERMIN√â\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. √âvaluation Finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä √âVALUATION FINALE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Charger le meilleur mod√®le\n",
    "checkpoint = torch.load(CONFIG[\"checkpoint_dir\"] / \"best_model.pth\", weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\n‚úÖ Statistiques du meilleur mod√®le:\")\n",
    "print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "print(f\"  Val Accuracy: {checkpoint['val_acc']:.2f}%\")\n",
    "print(f\"  Val Loss: {checkpoint.get('val_loss', 'N/A')}\")\n",
    "\n",
    "# √âvaluation sur le set de validation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"√âvaluation sur Validation Set\")\n",
    "print(\"=\"*80)\n",
    "all_preds_val = []\n",
    "all_labels_val = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "        images = images.to(CONFIG[\"device\"])\n",
    "        \n",
    "        if CONFIG[\"use_amp\"]:\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(pixel_values=images)\n",
    "        else:\n",
    "            outputs = model(pixel_values=images)\n",
    "            \n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        all_preds_val.extend(predictions.cpu().numpy())\n",
    "        all_labels_val.extend(labels.numpy())\n",
    "\n",
    "val_acc_final = accuracy_score(all_labels_val, all_preds_val)\n",
    "print(f\"\\n‚úì Validation Accuracy: {val_acc_final*100:.2f}%\")\n",
    "\n",
    "# √âvaluation sur le hold-out test set\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"√âvaluation sur Hold-out Test Set (Final Benchmark)\")\n",
    "print(\"=\"*80)\n",
    "all_preds_test = []\n",
    "all_labels_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Hold-out Test\"):\n",
    "        images = images.to(CONFIG[\"device\"])\n",
    "        \n",
    "        if CONFIG[\"use_amp\"]:\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(pixel_values=images)\n",
    "        else:\n",
    "            outputs = model(pixel_values=images)\n",
    "            \n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        all_preds_test.extend(predictions.cpu().numpy())\n",
    "        all_labels_test.extend(labels.numpy())\n",
    "\n",
    "test_acc_final = accuracy_score(all_labels_test, all_preds_test)\n",
    "print(f\"\\n‚úì Hold-out Test Accuracy: {test_acc_final*100:.2f}%\")\n",
    "\n",
    "# Rapport de classification d√©taill√© (sur hold-out test)\n",
    "print(\"\\nüìã Rapport de Classification (Hold-out Test):\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(all_labels_test, all_preds_test, digits=4, zero_division=0))\n",
    "\n",
    "# Sauvegarder l'historique\n",
    "history_path = CONFIG[\"checkpoint_dir\"] / \"training_history.json\"\n",
    "with open(history_path, \"w\") as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "# Sauvegarder les r√©sultats finaux\n",
    "results = {\n",
    "    \"best_epoch\": int(checkpoint['epoch']),\n",
    "    \"val_acc\": float(checkpoint['val_acc']),\n",
    "    \"final_val_acc\": float(val_acc_final * 100),\n",
    "    \"final_test_acc\": float(test_acc_final * 100),\n",
    "    \"num_classes\": int(num_classes),\n",
    "    \"model\": CONFIG[\"model_name\"]\n",
    "}\n",
    "\n",
    "results_path = CONFIG[\"checkpoint_dir\"] / \"final_results.json\"\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Historique sauvegard√©: {history_path}\")\n",
    "print(f\"‚úÖ R√©sultats finaux sauvegard√©s: {results_path}\")\n",
    "print(f\"‚úÖ Meilleur mod√®le: {CONFIG['checkpoint_dir'] / 'best_model.pth'}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ √âVALUATION COMPL√àTE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sauvegarder les Graphiques Finaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er et sauvegarder les graphiques finaux\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot Loss\n",
    "axes[0].plot(history[\"train_loss\"], label='Train Loss', marker='o', linewidth=2)\n",
    "axes[0].plot(history[\"val_loss\"], label='Val Loss', marker='s', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training et Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Accuracy\n",
    "axes[1].plot(history[\"train_acc\"], label='Train Acc', marker='o', linewidth=2)\n",
    "axes[1].plot(history[\"val_acc\"], label='Val Acc', marker='s', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training et Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = CONFIG[\"checkpoint_dir\"] / 'training_curves.png'\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Graphiques sauvegard√©s: {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. R√©sum√© Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üìä R√âSUM√â DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_samples = len(df_dev) + len(df_holdout)\n",
    "\n",
    "print(f\"\\nDonn√©es:\")\n",
    "print(f\"  Total √©chantillons: {total_samples:,}\")\n",
    "print(f\"  Training: {len(train_dataset):,} (~{len(train_dataset)/total_samples*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(val_dataset):,} (~{len(val_dataset)/total_samples*100:.1f}%)\")\n",
    "print(f\"  Hold-out: {len(test_dataset):,} (15.0%)\")\n",
    "print(f\"  Classes: {num_classes}\")\n",
    "\n",
    "print(f\"\\nMod√®le:\")\n",
    "print(f\"  Architecture: {CONFIG['model_name']}\")\n",
    "print(f\"  Param√®tres entra√Ænables: {trainable_params:,}\")\n",
    "\n",
    "print(f\"\\nEntra√Ænement:\")\n",
    "print(f\"  Epochs compl√©t√©s: {len(history['train_loss'])}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "\n",
    "print(f\"\\nMeilleurs R√©sultats (sur Validation):\")\n",
    "print(f\"  Best epoch: {checkpoint['epoch']}\")\n",
    "print(f\"  Best val accuracy: {checkpoint['val_acc']:.2f}%\")\n",
    "\n",
    "print(f\"\\nR√©sultats Finaux:\")\n",
    "print(f\"  Final val accuracy: {val_acc_final*100:.2f}%\")\n",
    "print(f\"  Final hold-out test accuracy: {test_acc_final*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nFichiers sauvegard√©s:\")\n",
    "print(f\"  Checkpoint: {CONFIG['checkpoint_dir'] / 'best_model.pth'}\")\n",
    "print(f\"  Historique: {CONFIG['checkpoint_dir'] / 'training_history.json'}\")\n",
    "print(f\"  R√©sultats: {CONFIG['checkpoint_dir'] / 'final_results.json'}\")\n",
    "print(f\"  Configuration: {CONFIG['checkpoint_dir'] / 'config.json'}\")\n",
    "print(f\"  Graphiques: {CONFIG['checkpoint_dir'] / 'training_curves.png'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ ENTRA√éNEMENT PRODUCTION ViT TERMIN√â\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Finish WandB logging\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from timm.data.mixup import Mixup\n",
    "from timm.loss import SoftTargetCrossEntropy\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# 1. Update Checkpoint Directory\n",
    "CONFIG[\"checkpoint_dir\"] = Path(\"/workspace/checkpoints/vit_mixup_v3\")\n",
    "CONFIG[\"checkpoint_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Checkpoint directory set to: {CONFIG['checkpoint_dir']}\")\n",
    "\n",
    "# 2. Configure Mixup/CutMix\n",
    "mixup_args = {\n",
    "    'mixup_alpha': 0.8,       # Mixup alpha value\n",
    "    'cutmix_alpha': 1.0,      # CutMix alpha value\n",
    "    'cutmix_minmax': None,\n",
    "    'prob': 1.0,              # Probability of applying mixup or cutmix\n",
    "    'switch_prob': 0.5,       # Probability of switching to cutmix instead of mixup\n",
    "    'mode': 'batch',\n",
    "    'label_smoothing': 0.1,\n",
    "    'num_classes': 27\n",
    "}\n",
    "\n",
    "# Initialize Mixup\n",
    "mixup_fn = Mixup(**mixup_args)\n",
    "print(\"Mixup & CutMix initialized\")\n",
    "\n",
    "# 3. Define Loss Functions\n",
    "# Training: SoftTargetCrossEntropy (for mixed labels)\n",
    "criterion_train = SoftTargetCrossEntropy()\n",
    "# Validation: Standard CrossEntropy (for integer labels)\n",
    "criterion_val = nn.CrossEntropyLoss()\n",
    "\n",
    "# 4. Reset Optimizer and Scheduler\n",
    "# Note: Increased weight_decay to 0.05 for regularization\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=0.05)\n",
    "\n",
    "# Recalculate steps\n",
    "num_training_steps = len(train_loader) * CONFIG[\"num_epochs\"]\n",
    "num_warmup_steps = int(num_training_steps * CONFIG[\"warmup_ratio\"])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "print(\"Optimizer and Scheduler reset (Weight Decay = 0.05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Using standard text-based progress bar to avoid IProgress errors\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" STARTING TRAINING (V3 - MIXUP/CUTMIX)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Reset tracking variables\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "def update_plots_v3():\n",
    "    \"\"\"Real-time plotting of training curves.\"\"\"\n",
    "    clear_output(wait=True)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "    \n",
    "    # Plot Loss\n",
    "    axes[0].plot(history[\"train_loss\"], label='Train Loss', marker='o')\n",
    "    axes[0].plot(history[\"val_loss\"], label='Val Loss', marker='s')\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    # Note: Train Acc is dashed because it is not fully representative under Mixup\n",
    "    axes[1].plot(history[\"train_acc\"], label='Train Acc (Mixup)', marker='o', linestyle='--', alpha=0.5)\n",
    "    axes[1].plot(history[\"val_acc\"], label='Val Acc', marker='s', linewidth=2)\n",
    "    axes[1].set_title('Accuracy')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(f\" Current Best Val Acc: {best_val_acc:.2f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# Main Training Loop\n",
    "# ============================================================================\n",
    "for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
    "    \n",
    "    # --- 1. Training Phase ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Use ascii=True for compatibility with all terminals\n",
    "    train_pbar = tqdm(train_loader, desc=\"Training\", ascii=True)\n",
    "    \n",
    "    for images, labels in train_pbar:\n",
    "        images, labels = images.to(CONFIG[\"device\"]), labels.to(CONFIG[\"device\"])\n",
    "        \n",
    "        # ‚û§ Apply Mixup / CutMix\n",
    "        if mixup_fn is not None:\n",
    "            images, labels = mixup_fn(images, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # ‚û§ Forward Pass & Loss Calculation\n",
    "        if CONFIG[\"use_amp\"]:\n",
    "            # Mixed Precision Context\n",
    "            with torch.cuda.amp.autocast(): \n",
    "                # Note: We do not pass labels to the model here\n",
    "                # We calculate the Mixup Loss manually\n",
    "                outputs = model(pixel_values=images)\n",
    "                loss = criterion_train(outputs.logits, labels)\n",
    "            \n",
    "            # Backward Pass\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            # Standard Precision (FP32)\n",
    "            outputs = model(pixel_values=images)\n",
    "            loss = criterion_train(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "        scheduler.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # --- 2. Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    # Validation does not use Mixup and does not need gradients\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=\"Validation\", ascii=True)\n",
    "        for images, labels in val_pbar:\n",
    "            images, labels = images.to(CONFIG[\"device\"]), labels.to(CONFIG[\"device\"])\n",
    "            \n",
    "            if CONFIG[\"use_amp\"]:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(pixel_values=images)\n",
    "                    # Validation uses standard CrossEntropyLoss\n",
    "                    loss = criterion_val(outputs.logits, labels)\n",
    "            else:\n",
    "                outputs = model(pixel_values=images)\n",
    "                loss = criterion_val(outputs.logits, labels)\n",
    "                \n",
    "            val_loss += loss.item()\n",
    "            # Calculate Accuracy\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "            \n",
    "    val_acc = 100.0 * val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # --- 3. Logging & Saving ---\n",
    "    # We record 0 for train_acc as it is not meaningful under Mixup\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "    history[\"train_acc\"].append(0) \n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    \n",
    "    # Save Best Model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'epoch': epoch + 1,\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, CONFIG[\"checkpoint_dir\"] / \"best_model.pth\")\n",
    "        print(f\"‚úÖ New best model saved! Val Acc: {val_acc:.2f}%\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"‚è≥ No improvement ({patience_counter}/3). Best: {best_val_acc:.2f}%\")\n",
    "        \n",
    "    # Update plots\n",
    "    update_plots_v3()\n",
    "    \n",
    "    # Early Stopping Check\n",
    "    if patience_counter >= 3:\n",
    "        print(\"\\n‚ö†Ô∏è Early Stopping Triggered.\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ TRAINING COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}