Projet Rakuten â€“ Classification de Produits

Ce dÃ©pÃ´t rassemble le travail rÃ©alisÃ© autour de la classification de produits Rakuten, en utilisant les informations texte (dÃ©signation + description) et, dans un second temps, les images.
Lâ€™objectif est surtout pratique : comprendre le jeu de donnÃ©es, tester diffÃ©rentes idÃ©es de prÃ©traitement et construire un pipeline clair et reproductible.

ğŸ“ Structure du projet
rakuten/
â”œâ”€â”€ archive/
â”‚   â””â”€â”€ phase1_exploration_text/     # Code exploratoire archivÃ© (Phase 1)
â”œâ”€â”€ data/                            # Datasets Rakuten
â”‚   â”œâ”€â”€ X_train_update.csv
â”‚   â””â”€â”€ Y_train_CVw08PX.csv
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 00_text_exploration.ipynb             # Exploration initiale
â”‚   â”œâ”€â”€ 01_Text_Preprocessing_Benchmark.ipynb # Phase 1: Preprocessing
â”‚   â”œâ”€â”€ 02_Vectorization_Strategies.ipynb     # Phase 2: Vectorization
â”‚   â”œâ”€â”€ 03_Model_Selection.ipynb              # Phase 2: Model Selection
â”‚   â””â”€â”€ archive/                              # Anciens notebooks
â”œâ”€â”€ src/
â”‚   â””â”€â”€ rakuten_text/               # BibliothÃ¨que modulaire de ML texte
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ preprocessing.py        # âœ… Nettoyage de texte (Phase 1)
â”‚       â”œâ”€â”€ benchmark.py            # âœ… Benchmark preprocessing (Phase 1)
â”‚       â”œâ”€â”€ features.py             # âœ… Features manuelles (Phase 2)
â”‚       â”œâ”€â”€ vectorization.py        # âœ… TF-IDF/Count + weighting (Phase 2)
â”‚       â”œâ”€â”€ experiments.py          # âœ… ExpÃ©rimentations systÃ©matiques (Phase 2)
â”‚       â”œâ”€â”€ models.py               # âœ… Pipelines ML (Phase 2)
â”‚       â”œâ”€â”€ categories.py           # âœ… Mapping 27 catÃ©gories (Phase 2)
â”‚       
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ configs/                    # Configurations optimales
â”‚   â””â”€â”€ models/                     # ModÃ¨les entraÃ®nÃ©s
â””â”€â”€ README.md                       # Ce fichier
```

## ğŸ¯ Objectif

Classifier automatiquement les produits Rakuten dans **27 catÃ©gories** en utilisant :
- **Texte** : DÃ©signation + Description des produits
- **Images** : Photos des produits (phase en cours)

## ğŸ“Š Ã‰tat d'Avancement

### âœ… Phase 1 : PrÃ©traitement de Texte (TERMINÃ‰E)

**RÃ©sultats clÃ©s :**
- **Baseline raw** : F1 = 0.7919
- **Meilleure stratÃ©gie** : `final_text_cleaner()` â†’ **F1 = 0.8024** (+1.32%)
- **22 stratÃ©gies** de nettoyage comparÃ©es sur 84,916 Ã©chantillons

**Fonction de production :** `final_text_cleaner()` dans `src/rakuten_text/preprocessing.py`

**Notebook :** `notebooks/01_Text_Preprocessing_Benchmark.ipynb`

### âœ… Phase 2 : Vectorization & ModÃ¨les Texte (TERMINÃ‰E)

**RÃ©sultats clÃ©s :**
- **Configuration optimale** : TF-IDF Split + features manuelles + title weighting
- **Performance** : F1 = 0.8420 (+6.33% vs baseline)
- **HyperparamÃ¨tres** : max_features=20k, ngram_range=(1,2), split_size=0.15

**ExpÃ©rimentations rÃ©alisÃ©es :**
1. Count vs TF-IDF vectorization
2. Split vs Merged text strategies
3. Manual features extraction (24 features)
4. **Title weighting** (1x-3x importance)
5. Hyperparameter grid search
6. Model comparison (LogReg, SVM, XGBoost, RF)

**Modules crÃ©Ã©s :**
- `vectorization.py` : TF-IDF/Count + FeatureWeighter (title weighting)
- `features.py` : 24 features manuelles textuelles
- `experiments.py` : Framework complet d'expÃ©rimentation + tracking + reporting
- `models.py` : Pipelines ML (LogReg, SVM, XGBoost, RF)
- `categories.py` : Mapping 27 catÃ©gories + noms courts

**FonctionnalitÃ©s clÃ©s :**
- âœ… Title weighting automatique (1x-3x)
- âœ… Tracking global des scores (tous les modÃ¨les)
- âœ… VÃ©rification d'optimalitÃ© automatique
- âœ… GÃ©nÃ©ration de rapports formatÃ©s

**Notebooks :**
- `02_Vectorization_Strategies.ipynb`
- `03_Model_Selection.ipynb`

### ğŸ”„ Phase 3 : Traitement d'Images (EN COURS)

Exploration des features visuelles et architectures CNN/Transfer Learning.

### ğŸ“‹ Phase 4 : Fusion Multimodale (PLANIFIÃ‰E)

- Ensembles multi-modaux (texte + image)
- Fine-tuning de modÃ¨les transformer
- Optimisation hyperparamÃ¨tres


Les notebooks trop anciens ou expÃ©rimentaux sont dÃ©placÃ©s dans archive/ pour garder une arborescence propre.

ğŸ¯ Objectif gÃ©nÃ©ral

Le dataset contient environ 85k produits rÃ©partis dans 27 catÃ©gories.
La premiÃ¨re Ã©tape a Ã©tÃ© de mettre en place un prÃ©traitement du texte stable et facilement testable.
Les expÃ©rimentations sur les images sont en cours et seront ajoutÃ©es au fur et Ã  mesure.

ğŸ“Š Phase 1 â€” PrÃ©traitement du texte (terminÃ©e)

Le travail a portÃ© principalement sur :

la correction des problÃ¨mes dâ€™encodage,

la gestion des balises HTML,

la normalisation Unicode,

la suppression de ponctuation bruitÃ©e,

les stopwords franÃ§ais et anglais.

Au total, 22 configurations de nettoyage ont Ã©tÃ© comparÃ©es.

Quelques repÃ¨res :

Baseline (texte brut) : F1 = 0.7921

Meilleure stratÃ©gie : F1 = 0.8024

Le pipeline final est implÃ©mentÃ© dans
src/rakuten_text/preprocessing.py â†’ final_text_cleaner()

Notebook de rÃ©fÃ©rence :
notebooks/01_Text_Preprocessing_Benchmark.ipynb

ğŸ–¼ï¸ Phase 2 â€” Images (en cours)

Exploration des premiÃ¨res features visuelles (HOG, couleurs, downsampling)
et tests prÃ©liminaires avec quelques architectures CNN.
Rien nâ€™est encore figÃ© : câ€™est une phase de repÃ©rage.

ğŸ”§ Phase 3 â€” ModÃ©lisation (Ã  venir)

Combinaison texte + image

Tests de modÃ¨les plus modernes

Ajustement des hyperparamÃ¨tres

Ã‰ventuels ensembles multi-modaux

ğŸš€ Installation
git clone <url>
cd rakuten
pip install -r requirements.txt

# Stopwords pour NLTK
python -c "import nltk; nltk.download('stopwords')"

ğŸ“ Exemple dâ€™utilisation du nettoyage
from src.rakuten_text.preprocessing import final_text_cleaner

txt = "<p>Ordinateur portable HP 15.6 pouces - 299,99&nbsp;â‚¬</p>"
print(final_text_cleaner(txt))

âš™ï¸ Lancer un benchmark
from src.rakuten_text.benchmark import load_dataset, run_benchmark

df = load_dataset("data")
results = run_benchmark(df)
print(results.head())
